import random
from time import time

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

from squadro.agents.alphabeta_agent import AlphaBetaAgent

run_folder = './run/'
model_path = '../../model/contest_agent.pt'

class DeepNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        
        nin = 11             # 11 inputs: player id, 5 first numbers for the player 0 and five numbers for the player 1
        nout = 5             # 5 outputs: probability to choose one of the 5 actions
        hidden_layers = 200  # Size of the hidden layers

        self.batch_hid = nn.BatchNorm1d(num_features=hidden_layers)
        
        self.lin = nn.Linear(nin, hidden_layers)
        
        self.linp1 = nn.Linear(hidden_layers, hidden_layers)
        self.linp2 = nn.Linear(hidden_layers, hidden_layers)
        self.linp3 = nn.Linear(hidden_layers, nout)
        
        self.linv1 = nn.Linear(hidden_layers, hidden_layers)
        self.linv2 = nn.Linear(hidden_layers, hidden_layers)
        self.linv3 = nn.Linear(hidden_layers, 1)
        


    def forward(self, x):
        #print(x.shape)
        l = len(x.size())
        
        if l == 1:
            x = x.unsqueeze(0)
        
        x = F.relu(self.batch_hid(self.lin(x)))
        
        ph = F.relu(self.batch_hid(self.linp1(x)))
        ph = F.relu(self.batch_hid(self.linp2(ph)))
        ph = self.linp3(ph)
        softph = F.softmax(ph, dim=-1)
        
        vh = F.relu(self.batch_hid(self.linv1(x)))
        vh = F.relu(self.batch_hid(self.linv2(vh)))
        vh = self.linv3(vh)
        
        #print(x)
        #print(ph.shape)
        #print(vh.shape)
        #print(vh)
        #print(ph)
        return (softph, vh)

"""
Contest agent
"""


class ContestAgent(AlphaBetaAgent):

    def __init__(self):
        self.action_size = 5         # Number of actions
        self.max_depth = 9           # Max depth of the MCTS (to remove)
        self.max_time = 0            # Max time for the simulations
        self.start_time = 0          # Start time of the simulation
        self.total_time = 0          # Total time of the game
        self.mcts = None
        self.MC_steps = 70           # Number of steps in MCTS
        self.turn_time = 0.03        # Percentage of total time allowed for each turn
        self.hurry_time = 0.2        # Percentage of total time when it begins to hurry up
        self.epsilonMove = 0.03      # Probability to choose randomly the move
        self.epsilonMCTS = 0.2       # Probability to choose randomly the node in MCTS (for TRAINING only)
        self.tau = 1                 # If MCTS stochastic: select action with distribution pi^(1/tau)
        
        self.results = None          # Store results
        
        self.deepnetwork = DeepNetwork()
        #torch.save(self.deepnetwork.state_dict(), model_path)
        self.set_model_path(model_path)
        self.tensor_state = None

        #for param in self.deepnetwork.parameters():
        #    print(param.data)

        #print(self.deepnetwork)


    def get_name(self):
        return 'Group 13'
    
    
    def set_model_path(self, path):
        self.deepnetwork.load_state_dict(torch.load(path))
        self.deepnetwork.eval()
    
    """
    This is the smart class of an agent to play the Squadro game.
    """

    def get_action(self, state, last_action=None, time_left=None):
        self.last_action = last_action
        self.time_left = time_left
        self.start_time = time()
        if self.total_time == 0:
            self.total_time = time_left;
        if time_left / self.total_time > self.hurry_time:
            self.max_time = self.turn_time * self.total_time
        else:
            self.max_time = self.turn_time * self.total_time * (time_left / (self.hurry_time * self.total_time))**2
        best_move = 1
        
        root = Node(state)
        self.mcts = MCTS(root)
        
        #### MCTS
        root_value = self.evaluateLeaf(root, 0)
        n = 1
        #while time() - self.start_time < self.max_time: # TO REPLACE for contest
        while n < self.MC_steps:
            #print(time() - self.start_time)
            #logger_mcts.info('***************************')
            #logger_mcts.info('****** SIMULATION %d ******', n)
            #logger_mcts.info('***************************')
            self.simulate(state)
            n += 1
        #print("Finish")
        #print(self.current_depth)
        #print("Time elapsed during smart agent play:", time() - self.start_time)
        
        pi, values = self.getAV()
        
        #print(pi)
        
        #### pick the action (stochastically with prob = epsilon)
        tau = self.tau if random.uniform(0, 1) < self.epsilonMove else 0
        best_move, value = self.chooseAction(pi, values, tau)

        nextState, _ = self.takeAction(state, best_move)

        NN_value = -self.evaluate(nextState)[1] # - sign because it evaluates with respect to the current player of the state

        ##logger_mcts.info('ACTION VALUES...%s', pi)
        ##logger_mcts.info('CHOSEN ACTION...%d', best_move)
        ##logger_mcts.info('MCTS PERCEIVED VALUE...%f', value)  # Value estimated by MCTS: Q = W/N (average of the all the values along the path)
        ##logger_mcts.info('NN PERCEIVED VALUE...%f', NN_value) # Value estimated by the Neural Network (only for the next state)

        #print(best_move, pi, value, NN_value)
      
        l1 = [state.get_pawn_advancement(0, pawn) for pawn in [0, 1, 2, 3, 4]]
        l2 = [state.get_pawn_advancement(1, pawn) for pawn in [0, 1, 2, 3, 4]]
        results = [self.id] + l1 + l2 + list(pi)
        self.results = np.array(results)
        self.results = np.transpose(np.reshape(self.results, newshape=[-1,1]))
        #print('{} {} {} {}'.format(self.id, l1, l2, pi))
        
        #print_state(state)
        
        return best_move
  
    
    def simulate(self, state):
      
        #logger_mcts.info('ROOT NODE...')
        #render(self.mcts.root.state, #logger_mcts)
        #logger_mcts.info('CURRENT PLAYER...%d', self.mcts.root.playerTurn)
        
        ##### MOVE TO LEAF NODE
        # Breadcrumbs = path from root to leaf
        leaf, done, breadcrumbs = self.mcts.move_to_leaf(self)
        #render(leaf.state, #logger_mcts)
        
        
        #print_state(leaf.state)

        ##### EVALUATE THE LEAF NODE with deep neural network + add edges to leaf node
        value = self.evaluateLeaf(leaf, done)

        

        ##### BACKFILL THE VALUE THROUGH THE TREE
        self.mcts.back_fill(leaf, value, breadcrumbs)


    def evaluateLeaf(self, leaf, done):

        #logger_mcts.info('------EVALUATING LEAF------')

        if done == 0:
    
            probs, value = self.evaluate(leaf.state)
            #print(probs)
            allowedActions = leaf.state.get_current_player_actions()
            #logger_mcts.info('PREDICTED VALUE FOR %d: %f', leaf.playerTurn, value)

            probs = probs[allowedActions]

            # Add node in tree
            for idx, action in enumerate(allowedActions):
                newState, _ = self.takeAction(leaf.state, action)
                node = Node(newState)
                #if newState.id not in self.mcts.tree:
                #self.mcts.addNode(node)
                #logger_mcts.info('added node......p = %f', probs[idx])
                #else:
                #    node = self.mcts.tree[newState.id]
                 #   #logger_mcts.info('existing node...%s...', node.id)

                newEdge = Edge(leaf, node, probs[idx], action)
                leaf.edges.append((action, newEdge))
                
        else: # End of game leaf
            value = 1 if leaf.state.cur_player == self.id else -1

        return value
    
    
    def getAV(self):
        edges = self.mcts.root.edges
        pi = np.zeros(self.action_size, dtype=np.integer)
        values = np.zeros(self.action_size, dtype=np.float32)
        
        for action, edge in edges:
            pi[action] = pow(edge.stats['N'], 1/self.tau)
            values[action] = edge.stats['Q']
        pi = pi / (np.sum(pi) * 1.0)
        return pi, values
      
      
    def chooseAction(self, pi, values, tau):
        if tau == 0: # Choose deterministically
            actions = np.argwhere(pi == max(pi))
            action = random.choice(actions)[0] # if several states have the same prob
        else: # Choose stochastically (for TRAINING)
            action_idx = np.random.multinomial(1, pi)
            action = np.where(action_idx==1)[0][0] # random action

        value = values[action]

        return action, value
        
    
    def takeAction(self, state, a):
      
        newState = state.copy()
        newState.apply_action(a)
        
        done = 1 if self.cutoff(newState, self.max_depth) else 0

        return newState, done


    """
    The successors function must return (or yield) a list of
    pairs (a, s) in which a is the action played to reach the
    state s.
    """
    def successors(self, state):
        actions = state.get_current_player_actions()
        for a in actions:
            s = state.copy()
            s.apply_action(a)
            yield (a, s)


    """
    The cutoff function returns true if the alpha-beta/minimax
    search has to stop and false otherwise.
    """
    def cutoff(self, state, depth):
        return state.game_over_check() #or time() - self.start_time > self.max_time


    """
    The evaluate function must return an integer value
    representing the utility function of the board.
    """
    def evaluate(self, state):
        l1 = [state.get_pawn_advancement(state.cur_player, pawn) for pawn in [0, 1, 2, 3, 4]]
        l2 = [state.get_pawn_advancement(1 - state.cur_player, pawn) for pawn in [0, 1, 2, 3, 4]]
        x = torch.FloatTensor([state.cur_player] + l1 + l2)
        ph, vh = self.deepnetwork(x)
        ph = ph.data.numpy()[0,:]
        vh = np.float(vh.data.numpy())
        return (ph, vh) # Deep neural network evaluation
    
        #ph = np.zeros(self.action_size)
        #for a, s in self.successors(state):
        #    ph[a] = self.sum_eval(s)
        #ph = np.exp(ph) / sum(np.exp(ph))
        #vh = sum(l1[1:]) - sum(l2[1:])
        #return (ph, vh) # basic eval function


    def sum_eval(self, state):
        l1 = [state.get_pawn_advancement(state.cur_player, pawn) for pawn in [0, 1, 2, 3, 4]]
        l2 = [state.get_pawn_advancement(1 - state.cur_player, pawn) for pawn in [0, 1, 2, 3, 4]]
        l1.sort()
        l2.sort()
        return sum(l1[1:]) - sum(l2[1:])

